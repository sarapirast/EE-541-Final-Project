{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpj/ouVoFQ2HrfpEogklOh"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ex5A8P4oKTOr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchtext.vocab import GloVe\n",
        "import json\n",
        "import math\n",
        "\n",
        "glove_vect= GloVe(name=\"6B\", dim=300)\n",
        "embedding_dim= 300\n",
        "\n",
        "with open(\"data/vocab.json\",\"r\") as f:\n",
        "  vocab= json.load(f)\n",
        "\n",
        "word2idx= vocab['word2idx']\n",
        "embeddings= len(word2idx)\n",
        "weight_matrix= torch.zeros((embeddings, embedding_dim))\n",
        "\n",
        "for word,i in word2idx.items():\n",
        "  try:\n",
        "    weight_matrix[i]= glove_vect[word]\n",
        "  except KeyError:\n",
        "    weight_matrix[i]= torch.randn(embedding_dim)\n",
        "\n",
        "\n",
        "embedding_layer= nn.Embedding.from_pretrained(\n",
        "    weight_matrix,\n",
        "    freeze=True,\n",
        "    padding_idx=word2idx[\"<pad>\"]\n",
        "    )\n",
        "\n",
        "def positionalencoding(max_len,d_model,device=None):\n",
        "  pe= np.zeros((max_len,d_model))\n",
        "  for pos in range(max_len):\n",
        "    for i in range(0,d_model,2):\n",
        "      theta= pos/ (10000**(i/d_model))\n",
        "      pe[pos,i]= np.sin(theta)\n",
        "      if i+1<d_model:\n",
        "        pe[pos,i+1]= math.cos(theta)\n",
        "\n",
        "  pe= torch.tensor(pe, dtype=torch.float32).unsqueeze(0)\n",
        "  if device is not None:\n",
        "    pe = pe.to(device)\n",
        "  return pe\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "  def __init__(self,num_classes,d_model=300,num_layers=2,nhead=4,max_len=512,pad_id=0,device=None):\n",
        "    super().__init__()\n",
        "    self.pad_id= pad_id\n",
        "    self.embedding= embedding_layer\n",
        "    pos_embedding= positionalencoding(max_len,d_model,device=device)\n",
        "    self.register_buffer(\"pos_embedding\",pos_embedding)\n",
        "    encoder_layer= nn.TransformerEncoderLayer(\n",
        "        d_model=d_model,\n",
        "        nhead=nhead,\n",
        "        dim_feedforward=256,\n",
        "        dropout=0.1,\n",
        "        batch_first=True ##(batch,seq,di)\n",
        "        )\n",
        "    self.encoder= nn.TransformerEncoder(\n",
        "        encoder_layer,\n",
        "        num_layers=num_layers\n",
        "        )\n",
        "    self.dropout= nn.Dropout(0.1)\n",
        "    self.fc= nn.Linear(d_model,num_classes)\n",
        "  def forward(self,x):\n",
        "    ##need pooling and masking for <pad>\n",
        "    src= (x==self.pad_id)\n",
        "    x= self.embedding(x)\n",
        "    seq= x.size(1)\n",
        "    x+= self.pos_embedding[:,:seq,:]\n",
        "    encoder_output= self.encoder(x,src_key_padding_mask=src)\n",
        "    mask= ~src\n",
        "    mask= mask.unsqueeze(-1)\n",
        "    encoder_output= encoder_output.masked_fill(~mask,float('-inf'))\n",
        "    s,_ = encoder_output.max(dim=1)\n",
        "    logits= self.fc(s)\n",
        "    return logits\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = TransformerModel(\n",
        "    num_classes=embeddings,\n",
        "    d_model=300,\n",
        "    num_layers=2,\n",
        "    nhead=4,\n",
        "    max_len=40,\n",
        "    pad_id=word2idx[\"<pad>\"],\n",
        "    device=device,\n",
        ").to(device)"
      ]
    }
  ]
}